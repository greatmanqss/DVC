#include <algorithm>
#include <vector>

#include "caffe/layer.hpp"
#include "caffe/layers/bound_triplet_loss.hpp"
#include "caffe/util/io.hpp"
#include "caffe/util/math_functions.hpp"

namespace caffe {

  template <typename Dtype>
  void BoundTripletLossLayer<Dtype>::LayerSetUp(
  const vector<Blob<Dtype>*>& bottom, const vector<Blob<Dtype>*>& top) {
  LossLayer<Dtype>::LayerSetUp(bottom, top);
  CHECK_EQ(bottom[0]->height(), 1);//check every dim's size, bottom[0]->data, bottom[1]->label
  CHECK_EQ(bottom[0]->width(), 1);
  CHECK_EQ(bottom[1]->channels(), 1);
  CHECK_EQ(bottom[1]->height(), 1);
  CHECK_EQ(bottom[1]->width(), 1);
  diff_.Reshape(1, bottom[0]->channels(), 1, 1);//store bi-bj
  dist_norm.Reshape(1, 1, bottom[0]->num(), bottom[0]->num());//store ||bi - bj||
  trip_loss.Reshape(1, 1, bottom[0]->num(), bottom[0]->num());// store the loss of triplet generated by i,j
 }


  template <typename Dtype>
  void BoundTripletLossLayer<Dtype>::Forward_cpu(
						const vector<Blob<Dtype>*>& bottom,
						const  vector<Blob<Dtype>*>& top) {

    const Dtype* bottom_data = bottom[0]->cpu_data();
    const Dtype* label = bottom[1]->cpu_data();
    const int num = bottom[0]->num();//batch_size
    const int dim = bottom[0]->channels();//length of bit
	
    //int label_dim = bottom[1]->count() / bottom[1]->num();
    Dtype margin = this->layer_param_.bound_triplet_loss_param().margin();//triplet margin
	//Dtype tradeoff = this->layer_param_.bound_triplet_loss_param().tradeoff();//balance param between triplet & the same class loss
	//Dtype tradeoff2 = this->layer_param_.bound_triplet_loss_param().tradeoff2();//balance param between discriminative loss and binary loss
	bool sim;
    Dtype loss(0.0);//overall loss
	//Dtype reg(0.0);//binary loss
	//Dtype data(0.0);// one bit data
	caffe_set(num*num, Dtype(0), dist_norm.mutable_cpu_data());//init the dist matrix
	caffe_set(num*num, Dtype(0), trip_loss.mutable_cpu_data());//init the trip loss matrix 
    n_tri = 0;// # of triplet
	
	
    for (int i = 0; i < num; ++i) {
      for (int j = i+1; j < num; ++j) {
		caffe_sub(dim, bottom_data+(i*dim), bottom_data+(j*dim), diff_.mutable_cpu_data());//b_i - b_j
		Dtype dist_sq = caffe_cpu_dot(dim, diff_.cpu_data(), diff_.cpu_data());  //D_w^2
        *(dist_norm.mutable_cpu_data()+(i*num+j)) = dist_sq;
		*(dist_norm.mutable_cpu_data()+(j*num+i)) = dist_sq;//update the dist matrix(symmetrical)
		
        sim = ((static_cast<int>(label[i])) == (static_cast<int>(label[j])));
		Dtype single_trip_loss(0.0);
		if(!sim)
			continue;
		n_tri += 1;
		
        for (int k = 0; k < num; ++k) {//calc the log loss in Jij
			sim = ((static_cast<int>(label[i])) == (static_cast<int>(label[k])));
			if(sim)
				continue;	  
            
	        Dtype norm2 = 0, norm2_p = 0;
			caffe_sub(dim, bottom_data+(i*dim), bottom_data+(k*dim), diff_.mutable_cpu_data());//b_i - b_k
			norm2 = caffe_cpu_dot(dim, diff_.cpu_data(), diff_.cpu_data());
			caffe_sub(dim, bottom_data+(j*dim), bottom_data+(k*dim), diff_.mutable_cpu_data());//b_j - b_l
			norm2_p = caffe_cpu_dot(dim, diff_.cpu_data(), diff_.cpu_data());
			
			single_trip_loss += exp(margin-norm2);
			single_trip_loss += exp(margin-norm2_p);    
	    }
		if(single_trip_loss > 0)//make sure it has different class loss 
		   single_trip_loss = log(single_trip_loss);
	   
		single_trip_loss += dist_sq;// add the same class loss in Jij, maybe Jij only has the same class loss 
		*(trip_loss.mutable_cpu_data()+(i*num+j)) = single_trip_loss;
		*(trip_loss.mutable_cpu_data()+(j*num+i)) = single_trip_loss;
		
		loss += std::max(single_trip_loss,Dtype(0.0));
		//loss += tradeoff * dist_sq;//add the same class loss in overall loss J
		
      }
	  
	  /*for (int bit = 0; bit < dim; bit++){
		data = bottom_data[i*dim + bit]; 
        data = std::abs(data) - 1;
        reg += std::abs(data);		
	  }*/
    }
	if(n_tri > 0)//sample pair with same label exist
	   loss = loss / 2.0 / static_cast<Dtype>(n_tri);//div the 2*|P|
   
    //loss += tradeoff2 * reg / static_cast<Dtype>(num);//add the binary loss
	
    top[0]->mutable_cpu_data()[0] = loss;
   
  }

  template <typename Dtype>
  void BoundTripletLossLayer<Dtype>::Backward_cpu(const vector<Blob<Dtype>*>& top,
						 const vector<bool>& propagate_down, const vector<Blob<Dtype>*>& bottom) {
    const Dtype* bottom_data = bottom[0]->cpu_data();
    const Dtype* label = bottom[1]->cpu_data();
    Dtype* diff = bottom[0]->mutable_cpu_diff();
    memset(diff, 0, bottom[0]->count()*sizeof(Dtype));//init the diff 
    const int num = bottom[0]->num();
    const int dim = bottom[0]->channels();
	const Dtype alpha = top[0]->cpu_diff()[0] / static_cast<Dtype>(n_tri) ;//loss term's normlization's size, |P| 
    //const Dtype beta = top[0]->cpu_diff()[0] / static_cast<Dtype>(bottom[0]->num());//reg term's normalization's size, n 
	
    //int label_dim = bottom[1]->count() / bottom[1]->num();
    Dtype margin = this->layer_param_.bound_triplet_loss_param().margin();//triplet loss margin 
	//Dtype tradeoff = this->layer_param_.bound_triplet_loss_param().tradeoff();//balance param triplet & same class pair loss 
	//Dtype tradeoff2 = this->layer_param_.bound_triplet_loss_param().tradeoff2();//balance discriminative loss & reg loss 
	bool sim;
	//Dtype data(0.0);// one bit data

    for (int i = 0; i < num; ++i) {
	  /*for (int d = 0; d < dim; d++) {// reg term gradient
        data = bottom_data[i*dim + d];
      // gradient corresponding to the regularizer
        diff[i*dim + d] += tradeoff2 * beta * 
		(((data>=Dtype(1.0))||(data<=Dtype(0.0)&&data>=Dtype(-1.0)))?Dtype(1.0):Dtype(-1.0));
      }*/		
      for (int j = i+1; j < num; ++j) {
	    sim = ((static_cast<int>(label[i])) == (static_cast<int>(label[j])));
		if(!sim)
			continue;
		//pair same class gradient w.r.t bi, bj 
		caffe_sub(dim, bottom_data+(i*dim), bottom_data+(j*dim), diff_.mutable_cpu_data());//bi-bj
		//caffe_cpu_axpby(dim, alpha*tradeoff, diff_.cpu_data(), Dtype(1.0), diff + (i*dim));
		//caffe_cpu_axpby(dim, -alpha*tradeoff, diff_.cpu_data(), Dtype(1.0), diff + (j*dim));
		
		Dtype single_trip_loss = *(trip_loss.cpu_data()+(i*num+j));// Jij
		if(single_trip_loss <= 0)
			continue;
		//triplet gradient w.r.t bi, bj 
		caffe_cpu_axpby(dim, alpha, diff_.cpu_data(), Dtype(1.0), diff + (i*dim));
		caffe_cpu_axpby(dim, -alpha, diff_.cpu_data(), Dtype(1.0), diff + (j*dim));
		
	    for (int k = 0; k < num; ++k) {//triplet gradient w.r.t bi, bk, bj, bl
	        sim = ((static_cast<int>(label[i])) == (static_cast<int>(label[k])));
			if(sim)
				continue;	  
	        Dtype dist_ik = *(dist_norm.cpu_data()+(i*num+k));
			Dtype dist_jk = *(dist_norm.cpu_data()+(j*num+k));
			Dtype dist_ij = *(dist_norm.cpu_data()+(i*num+j));
			Dtype deriva_ik = -exp(margin-dist_ik)/exp(single_trip_loss-dist_ij);
			Dtype deriva_jk = -exp(margin-dist_jk)/exp(single_trip_loss-dist_ij);
			
			//triplet gradient w.r.t bi, bk
			caffe_sub(dim, bottom_data+(i*dim), bottom_data+(k*dim), diff_.mutable_cpu_data());//bi-bk
		    caffe_cpu_axpby(dim, alpha*deriva_ik, diff_.cpu_data(), Dtype(1.0), diff + (i*dim));
		    caffe_cpu_axpby(dim, -alpha*deriva_ik, diff_.cpu_data(), Dtype(1.0), diff + (k*dim));
			
			caffe_sub(dim, bottom_data+(j*dim), bottom_data+(k*dim), diff_.mutable_cpu_data());//bj-bl
		    caffe_cpu_axpby(dim, alpha*deriva_jk, diff_.cpu_data(), Dtype(1.0), diff + (j*dim)); 
		    caffe_cpu_axpby(dim, -alpha*deriva_jk, diff_.cpu_data(), Dtype(1.0), diff + (k*dim));
	       
	    }
      } 
	  
    }
    

  }


#ifdef CPU_ONLY
    STUB_GPU(BoundTripletLossLayer);
#endif

    INSTANTIATE_CLASS(BoundTripletLossLayer);
    REGISTER_LAYER_CLASS(BoundTripletLoss);

  }  // namespace caffe
